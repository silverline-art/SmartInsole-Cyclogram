#!/usr/bin/env python3
"""
CSV Standardization Script for Insole Gait Analysis
====================================================

Purpose:
    Convert all CSV files from DEMC-DATA directory to standardized 87-column format
    matching the reference format (10MWT.csv - Version 23).

Input:
    Source: /home/shivam/Desktop/Human_Pose/PROJECT CYCLOGRAM/Dataset/RAW-DATA/DEMC-DATA/*.csv

Output:
    Target: /home/shivam/Desktop/Human_Pose/PROJECT CYCLOGRAM/insole-sample-DEMC/

Format Conversion:
    - Version 20/22/23 (3-line header, variable cols) â†’ Version 23 (3-line header, 87 cols)
    - Version 207 (5-line header, 39 cols) â†’ Version 23 (3-line header, 87 cols)
    - Converts -1 placeholder values to 0 for GYRO columns (DEMC format)
    - Adds missing GYRO sensors (default: 0)
    - Adds missing pressure sensors (L/R_value5/6, default: 0)
    - Adds missing calibration columns (*_raw2, *_N2, default: 0)
    - Adds missing COP and temperature columns (default: calculated or 0)

Author: Generated by Claude Code
Date: 2025-10-22
Version: 2.1 - DEMC GYRO placeholder fix + path updates
"""

import os
import csv
import sys
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import numpy as np


class InsoleCSVStandardizer:
    """Standardizes insole CSV files to Version 23 87-column format."""

    # Reference 87-column format (10MWT.csv)
    REFERENCE_COLUMNS = [
        'timestamp', 'L_value1', 'L_value2', 'L_value3', 'L_value4',
        'R_value1', 'R_value2', 'R_value3', 'R_value4',
        'COP_Left', 'COP_Right', 'COP_Front', 'COP_Back',
        'COP_Left_Front', 'COP_Left_Back', 'COP_Right_Front', 'COP_Right_Back',
        'L_ACC_X', 'L_ACC_Y', 'L_ACC_Z', 'R_ACC_X', 'R_ACC_Y', 'R_ACC_Z',
        'L_value1_raw', 'L_value2_raw', 'L_value3_raw', 'L_value4_raw',
        'R_value1_raw', 'R_value2_raw', 'R_value3_raw', 'R_value4_raw',
        'L_sensor_1N', 'L_sensor_2N', 'L_sensor_3N', 'L_sensor_4N',
        'R_sensor_1N', 'R_sensor_2N', 'R_sensor_3N', 'R_sensor_4N',
        'L_value5', 'L_value6', 'R_value5', 'R_value6',
        'L_value5_raw', 'L_value6_raw', 'R_value5_raw', 'R_value6_raw',
        'L_sensor_5N', 'L_sensor_6N', 'R_sensor_5N', 'R_sensor_6N',
        'L_GYRO_X', 'L_GYRO_Y', 'L_GYRO_Z', 'R_GYRO_X', 'R_GYRO_Y', 'R_GYRO_Z',
        'L_TEMPERATURE', 'R_TEMPERATURE',
        'COP_Left_Left', 'COP_Left_Right', 'COP_Right_Left', 'COP_Right_Right',
        'L_value1_raw2', 'L_value2_raw2', 'L_value3_raw2', 'L_value4_raw2',
        'L_value5_raw2', 'L_value6_raw2',
        'R_value1_raw2', 'R_value2_raw2', 'R_value3_raw2', 'R_value4_raw2',
        'R_value5_raw2', 'R_value6_raw2',
        'L_sensor_1N2', 'L_sensor_2N2', 'L_sensor_3N2', 'L_sensor_4N2',
        'L_sensor_5N2', 'L_sensor_6N2',
        'R_sensor_1N2', 'R_sensor_2N2', 'R_sensor_3N2', 'R_sensor_4N2',
        'R_sensor_5N2', 'R_sensor_6N2'
    ]

    def __init__(self, source_dir: str, target_dir: str):
        """
        Initialize standardizer.

        Args:
            source_dir: Path to INSOLE-DATA directory
            target_dir: Path to output directory (Temp-csv)
        """
        self.source_dir = Path(source_dir)
        self.target_dir = Path(target_dir)

        # Ensure target directory exists
        self.target_dir.mkdir(parents=True, exist_ok=True)

        # Statistics
        self.processed = 0
        self.failed = 0
        self.skipped = 0
        self.format_stats = {'39col': 0, '63col': 0, '87col': 0}

    def detect_format(self, csv_path: Path) -> Tuple[Optional[int], Optional[List[str]]]:
        """
        Detect CSV format version and column headers.

        Returns:
            Tuple of (version_number, column_headers) or (None, None) if detection failed
        """
        try:
            with open(csv_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            if len(lines) < 4:
                return (None, None)

            # Check if line 0 contains "version" (Version 207 or DEMC format)
            if 'version' in lines[0].lower():
                # Get version number from line 1
                version = int(lines[1].strip()) if lines[1].strip().isdigit() else None

                # Try line 2 first (DEMC/Version 22/23 format)
                if len(lines) > 2 and ',' in lines[2]:
                    headers = lines[2].strip().split(',')
                # Fall back to line 4 (Version 207 format)
                elif len(lines) > 4:
                    headers = lines[4].strip().split(',')
                else:
                    headers = None

                return (version, headers)

            # Otherwise check if line 1 contains "version" (Alternative Version 22/23 format)
            elif 'version' in lines[1].lower():
                # Version 22/23: Line 1 = "version", Line 2 = version number, Line 3 = headers
                version = int(lines[2].strip()) if lines[2].strip().isdigit() else None
                headers = lines[3].strip().split(',') if len(lines) > 3 else None
                return (version, headers)

        except Exception as e:
            print(f"  âš  Error detecting format: {csv_path.name} - {e}")

        return (None, None)

    def create_column_mapping(self, source_headers: List[str]) -> Dict[str, int]:
        """
        Create mapping from reference columns to source column indices.

        Args:
            source_headers: List of column names from source file

        Returns:
            Dict mapping reference column names to source indices (-1 if not found)
        """
        mapping = {}
        for ref_col in self.REFERENCE_COLUMNS:
            try:
                mapping[ref_col] = source_headers.index(ref_col)
            except ValueError:
                mapping[ref_col] = -1  # Column not found in source
        return mapping

    def standardize_row(self, source_row: List[str], column_mapping: Dict[str, int]) -> List[str]:
        """
        Standardize a single data row to 87-column format.

        Args:
            source_row: Row data from source file
            column_mapping: Mapping from reference columns to source indices

        Returns:
            Standardized row with 87 columns
        """
        standardized_row = []

        for ref_col in self.REFERENCE_COLUMNS:
            source_idx = column_mapping[ref_col]

            if source_idx >= 0 and source_idx < len(source_row):
                # Column exists in source - use its value
                value = source_row[source_idx].strip()

                # Convert -1 placeholder values to 0 for GYRO columns (DEMC data uses -1 for missing gyro)
                if 'GYRO' in ref_col and value == '-1':
                    standardized_row.append('0')
                else:
                    standardized_row.append(value)
            else:
                # Column missing in source - add default value
                standardized_row.append('0')

        return standardized_row

    def convert_to_87_columns(self, source_path: Path, target_path: Path,
                              version: int, headers: List[str]) -> bool:
        """
        Convert any format to 87-column Version 23 format.

        Args:
            source_path: Input CSV file
            target_path: Output CSV file
            version: Detected version number
            headers: Source column headers

        Returns:
            True if successful, False otherwise
        """
        try:
            # Read source file
            with open(source_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # Determine data start line based on header position
            # Check if headers are on line 2 (DEMC format) or line 4 (Version 207)
            with open(source_path, 'r', encoding='utf-8') as f:
                test_lines = f.readlines()

            if len(test_lines) > 2 and ',' in test_lines[2] and 'timestamp' in test_lines[2]:
                data_start_line = 3  # DEMC: headers on line 2, data starts line 3
            elif version == 207:
                data_start_line = 5  # Version 207: lines 0-4 are headers
            else:
                data_start_line = 4  # Version 22/23: lines 0-3 are headers

            # Create column mapping
            column_mapping = self.create_column_mapping(headers)

            # Track format type
            num_source_cols = len(headers)
            if num_source_cols == 39:
                self.format_stats['39col'] += 1
                format_type = '39-col (no GYRO)'
            elif num_source_cols == 63:
                self.format_stats['63col'] += 1
                format_type = '63-col (full sensors)'
            elif num_source_cols == 87:
                self.format_stats['87col'] += 1
                format_type = '87-col (already complete)'
            else:
                format_type = f'{num_source_cols}-col (unknown)'

            print(f"    Format: {format_type} â†’ 87-col")

            # Build standardized file
            standardized_lines = []

            # Lines 0-2: Header structure (DEMC format)
            standardized_lines.append("version\n")
            standardized_lines.append("23\n")
            standardized_lines.append(','.join(self.REFERENCE_COLUMNS) + '\n')

            # Lines 4+: Standardized data rows
            for line in lines[data_start_line:]:
                line = line.strip()
                if not line:
                    continue

                source_row = line.split(',')
                standardized_row = self.standardize_row(source_row, column_mapping)
                standardized_lines.append(','.join(standardized_row) + '\n')

            # Write to target
            with open(target_path, 'w', encoding='utf-8') as f:
                f.writelines(standardized_lines)

            return True

        except Exception as e:
            print(f"  âœ— Conversion failed: {source_path.name} - {e}")
            return False

    def standardize_file(self, source_path: Path) -> bool:
        """
        Standardize a single CSV file.

        Args:
            source_path: Path to source CSV file

        Returns:
            True if successful, False otherwise
        """
        # Detect format version and headers
        version, headers = self.detect_format(source_path)

        if version is None or headers is None:
            print(f"  âš  Could not detect format: {source_path.name}")
            self.skipped += 1
            return False

        # Check for GYRO sensors (critical for full analysis)
        num_cols = len(headers)
        has_gyro = any('GYRO' in col for col in headers)

        if num_cols == 39 or not has_gyro:
            print(f"  âš  SKIPPING (missing GYRO sensors): {source_path.name}")
            print(f"    Format: {num_cols}-col (no GYRO) - excluded from standardization")
            self.skipped += 1
            return False

        # Target file path
        target_path = self.target_dir / source_path.name

        # Convert to 87-column format
        print(f"  ðŸ”„ Converting Version {version}: {source_path.name}")
        success = self.convert_to_87_columns(source_path, target_path, version, headers)

        if success:
            self.processed += 1
            print(f"    âœ… Saved to: {target_path.name}")
        else:
            self.failed += 1

        return success

    def standardize_all(self) -> Tuple[int, int, int]:
        """
        Standardize all CSV files in source directory.

        Returns:
            Tuple of (processed_count, failed_count, skipped_count)
        """
        # Find all CSV files
        csv_files = sorted(self.source_dir.glob("*.csv"))

        if not csv_files:
            print(f"âš  No CSV files found in: {self.source_dir}")
            return (0, 0, 0)

        print(f"Found {len(csv_files)} CSV files in {self.source_dir.name}/")
        print("=" * 70)

        # Process each file
        for csv_file in csv_files:
            print(f"\nðŸ“„ Processing: {csv_file.name}")
            self.standardize_file(csv_file)

        # Summary
        print("\n" + "=" * 70)
        print("STANDARDIZATION SUMMARY")
        print("=" * 70)
        print(f"âœ… Successfully processed: {self.processed}")
        print(f"âœ— Failed: {self.failed}")
        print(f"âš  Skipped: {self.skipped}")
        print(f"ðŸ“Š Total: {len(csv_files)}")
        print()
        print("Format distribution:")
        print(f"  39-column files (no GYRO): {self.format_stats['39col']}")
        print(f"  63-column files (full sensors): {self.format_stats['63col']}")
        print(f"  87-column files (already complete): {self.format_stats['87col']}")
        print()
        print(f"ðŸ“‚ Output directory: {self.target_dir}")
        print("âœ… All files standardized to 87-column Version 23 format")

        return (self.processed, self.failed, self.skipped)


def main():
    """Main execution function."""

    # Define paths
    PROJECT_ROOT = Path("/home/shivam/Desktop/Human_Pose/PROJECT CYCLOGRAM")
    SOURCE_DIR = PROJECT_ROOT / "Dataset" / "RAW-DATA" / "DEMC-DATA"
    TARGET_DIR = PROJECT_ROOT / "insole-sample-DEMC"

    print("=" * 70)
    print("INSOLE CSV STANDARDIZATION SCRIPT v2.0")
    print("=" * 70)
    print(f"Source: {SOURCE_DIR}")
    print(f"Target: {TARGET_DIR}")
    print(f"Target Format: 87 columns (Version 23)")
    print("=" * 70)

    # Validate source directory
    if not SOURCE_DIR.exists():
        print(f"âœ— ERROR: Source directory not found: {SOURCE_DIR}")
        sys.exit(1)

    # Create standardizer and run
    standardizer = InsoleCSVStandardizer(
        source_dir=str(SOURCE_DIR),
        target_dir=str(TARGET_DIR)
    )

    processed, failed, skipped = standardizer.standardize_all()

    # Exit code
    if failed > 0:
        print(f"\nâš  Exiting with {failed} failures")
        sys.exit(1)
    elif processed == 0:
        print("\nâš  No files processed")
        sys.exit(1)
    else:
        print("\nâœ… All files standardized successfully to 87-column format")
        sys.exit(0)


if __name__ == "__main__":
    main()
